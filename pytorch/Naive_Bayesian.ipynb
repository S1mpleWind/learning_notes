{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d8168f-e388-4618-a90d-5227ad2d0ed1",
   "metadata": {},
   "source": [
    "# Naive Bayesian Algorithm\n",
    "---\n",
    "## Bayesian Algorithm\n",
    "\n",
    "Formula \n",
    "$$\n",
    "P(A \\vert B) = \\frac{P(B \\vert A)}{P(B)} \\cdot P(A)\n",
    "$$\n",
    "\n",
    "where : \n",
    "\n",
    "- $P(A)$ : prior\n",
    "- $P(B|A)$ ：likelihood\n",
    "- $P(A|B)$ : posterior\n",
    "- $P(B)$ : envidence\n",
    "\n",
    "It is natural that \n",
    "$$P(A|B) \\propto P(B|A)\\,P(A)\n",
    "$$\n",
    "\n",
    "But what is $P(B)$ for ? \n",
    "\n",
    "Let's do some tranformation\n",
    "$$\n",
    "P(A \\vert B) \\cdot P(B) = P(B \\vert A) \\cdot P(A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595fe8c-b856-45bb-a9b7-340f8a08931f",
   "metadata": {},
   "source": [
    "Here we can see that $left = P(AB) = right$ , this statement is true\n",
    "\n",
    "So $P(B)$ is used to *Probability Normalization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2720e4-a529-41ef-be36-27baa02faec0",
   "metadata": {},
   "source": [
    "---\n",
    "## Naive Bayesian\n",
    "\n",
    "Assume that we have a collection $\\small{X = \\{x_1, x_2, \\ldots, x_n\\}}$ and a class $\\small{C}$ , under the condition of **independency** , we have:\n",
    "$$\n",
    "P(X \\vert C) = P(x_1 \\vert C) \\cdot P(x_2 \\vert C) \\cdot \\ldots \\cdot P(x_n \\vert C)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c46054-48a9-42db-87f4-d2f709120d7f",
   "metadata": {},
   "source": [
    "---\n",
    "## Apply the algorithm\n",
    "\n",
    "### 1)Train\n",
    "1. Calculate the `Prior`\n",
    "\n",
    "$$\n",
    "P(C_{i}) = \\frac{n_{i}}{n}\n",
    "$$\n",
    "\n",
    "2. Calculate `likelihood`\n",
    "$$P(x_{j} \\vert C_{i}) = \\frac{n_{i,j}}{n_{i}}\n",
    "$$\n",
    "\n",
    "### 2)Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ced1f-0090-487c-9180-142388d7c457",
   "metadata": {},
   "source": [
    "Given an unsorted set $X$, whose *feature*` are $[x_1 , x_2, ... , x_d]$ . We can calculate the **posterior** probability for each class using Bayes’ theorem:\n",
    "\n",
    "$$\n",
    "P(C_{i} \\mid X) = \\frac{P(X \\mid C_{i})}{P(X)} \\cdot P(C_{i})\n",
    "$$\n",
    "\n",
    "\n",
    "($P(C_{i} \\mid X)$ stands for the chance that $X$ belongs to class $C_i$)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Here, $P(X)$ is a constant with respect to the class $C_i$ , so we can ignore it when comparing classes.  \n",
    "\n",
    "Now, by applying the **independence assumption** (as in Naive Bayes), the likelihood term $P(X \\mid C_i)$ can be factorized into the product of individual feature likelihoods:\n",
    "\n",
    "$$\n",
    "P(C_{i} \\mid X) \\propto P(C_{i}) \\cdot P(x_1 \\mid C_{i}) \\cdot P(x_2 \\mid C_{i}) \\cdot \\ldots \\cdot P(x_d \\mid C_{i})\n",
    "$$\n",
    "\n",
    "\n",
    "( $P(x_j \\mid C_{i})$ : the probability that the $j$-th feature takes value $x_j$        given class $C_i$ )\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "This shows that the posterior is proportional to the prior of the class multiplied by the product of the conditional probabilities of each feature given the class.\n",
    "\n",
    "When ***posterior*** reaches the peak in class $C_i$ , we say that $X$ belongs to $C_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920366c3-808b-44d9-8ed8-8796b8eec7c1",
   "metadata": {},
   "source": [
    "### 3) Code\n",
    "Use *iris* as example too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc9a540-a886-4b0d-adf2-b254b33779ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c61e2-b95a-4c47-9a3d-f6826b354efb",
   "metadata": {},
   "source": [
    "In training process , we need to obtain the *label* and corresponding *prior* of every class.\n",
    "In addition , we gonna calculate the *likelihood* using the **naive assumption**\n",
    "\n",
    "ps. we need to do some discrete processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95edc4ac-2286-48d1-a6e1-c57695312dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def naive_bayes_fit(X, y):\n",
    "    \"\"\"\n",
    "    :param X: sample features\n",
    "    :param y: sample labels\n",
    "    :returns: tuple (prior, likelihoods)\n",
    "    \"\"\"\n",
    "    # calculate prior probabilities for each class\n",
    "    clazz_labels, clazz_counts = np.unique(y, return_counts=True)\n",
    "    prior_probs = pd.Series({k: v / y.size for k, v in zip(clazz_labels, clazz_counts)})\n",
    "    \n",
    "    # make a copy of X\n",
    "    X = np.copy(X)\n",
    "    \n",
    "    likelihoods = {}\n",
    "    for j in range(X.shape[1]):  # loop over features , shape [1] stands for the columes (number of features)\n",
    "        \n",
    "        # discretize the feature values using equal-width binning and relabel them to 1~5\n",
    "        X[:, j] = pd.cut(X[:, j], bins=5, labels=np.arange(1, 6))\n",
    "        \n",
    "        for i in prior_probs.index:  # loop over classes\n",
    "            # filter samples by class(i)  and collect the values of this feature(j)\n",
    "            x_prime = X[y == i, j]\n",
    "            \n",
    "            # count the frequency of each unique feature value\n",
    "            x_values, x_counts = np.unique(x_prime, return_counts=True)\n",
    "            \n",
    "            for k, value in enumerate(x_values):  # loop over unique feature values\n",
    "                # compute the likelihood and store it in a dictionary\n",
    "                # the dictionary key is a tuple (class, feature index, feature value)\n",
    "                likelihoods[(i, j, value)] = x_counts[k] / x_prime.size\n",
    "    return prior_probs, likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a52d1-5e2c-4ebd-9809-a86233ac1592",
   "metadata": {},
   "source": [
    "by calling the function , we get a binary array ,\n",
    "which contains the `prior` of class $\\small{C_{i}}$  and the `likelihood` of  the $\\small{j}$th featurein class $\\small{C_{i}}$ owning`value`\n",
    "（we used `cut` function in pandas to cut `value` into `1` to `5`）\n",
    "the former one is a  `Series` object , while the later one is a `dict` object , as is shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cacfc6-4389-423d-bf75-7be486121029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior: \n",
      "0    0.333333\n",
      "1    0.333333\n",
      "2    0.333333\n",
      "dtype: float64\n",
      "likelihood: \n",
      "{(0, 0, np.float64(1.0)): np.float64(0.525), (0, 0, np.float64(2.0)): np.float64(0.45), (0, 0, np.float64(3.0)): np.float64(0.025), (1, 0, np.float64(1.0)): np.float64(0.05), (1, 0, np.float64(2.0)): np.float64(0.375), (1, 0, np.float64(3.0)): np.float64(0.425), (1, 0, np.float64(4.0)): np.float64(0.15), (2, 0, np.float64(1.0)): np.float64(0.025), (2, 0, np.float64(2.0)): np.float64(0.025), (2, 0, np.float64(3.0)): np.float64(0.45), (2, 0, np.float64(4.0)): np.float64(0.3), (2, 0, np.float64(5.0)): np.float64(0.2), (0, 1, np.float64(1.0)): np.float64(0.025), (0, 1, np.float64(3.0)): np.float64(0.325), (0, 1, np.float64(4.0)): np.float64(0.45), (0, 1, np.float64(5.0)): np.float64(0.2), (1, 1, np.float64(1.0)): np.float64(0.175), (1, 1, np.float64(2.0)): np.float64(0.325), (1, 1, np.float64(3.0)): np.float64(0.475), (1, 1, np.float64(4.0)): np.float64(0.025), (2, 1, np.float64(1.0)): np.float64(0.025), (2, 1, np.float64(2.0)): np.float64(0.35), (2, 1, np.float64(3.0)): np.float64(0.525), (2, 1, np.float64(4.0)): np.float64(0.05), (2, 1, np.float64(5.0)): np.float64(0.05), (0, 2, np.float64(1.0)): np.float64(1.0), (1, 2, np.float64(2.0)): np.float64(0.025), (1, 2, np.float64(3.0)): np.float64(0.525), (1, 2, np.float64(4.0)): np.float64(0.45), (2, 2, np.float64(4.0)): np.float64(0.525), (2, 2, np.float64(5.0)): np.float64(0.475), (0, 3, np.float64(1.0)): np.float64(0.975), (0, 3, np.float64(2.0)): np.float64(0.025), (1, 3, np.float64(2.0)): np.float64(0.125), (1, 3, np.float64(3.0)): np.float64(0.75), (1, 3, np.float64(4.0)): np.float64(0.125), (2, 3, np.float64(3.0)): np.float64(0.05), (2, 3, np.float64(4.0)): np.float64(0.525), (2, 3, np.float64(5.0)): np.float64(0.425)}\n"
     ]
    }
   ],
   "source": [
    "p_ci, p_x_ci = naive_bayes_fit(X_train, y_train)\n",
    "print('prior: ', p_ci, sep='\\n')\n",
    "print('likelihood: ', p_x_ci, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e634ee0-1e9d-4ea3-b70a-7f201f3e985a",
   "metadata": {},
   "source": [
    "\n",
    "illustration :\n",
    "\n",
    "\n",
    "take a look into the `likelihood` , first element \n",
    "\n",
    "`(0,0,1): 0.525` means that in class $C_0$ , the posibility of the no.$0$ feature having value `1` equals to `0.525`\n",
    "\n",
    "---\n",
    "\n",
    "Next we gonna use the result of the function to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c87bc90d-1689-4770-963e-0c26e2cf6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(X, p_ci, p_x_ci):\n",
    "    \"\"\"\n",
    "    naive_bayes predict\n",
    "    :param X: sample feature\n",
    "    :param p_ci: prior\n",
    "    :param p_x_ci: likelihood\n",
    "    :return: predicted value\n",
    "    \"\"\"\n",
    "    # discretize\n",
    "    X = np.copy(X)\n",
    "    for j in range(X.shape[1]):\n",
    "        X[:, j] = pd.cut(X[:, j], bins=5, labels=np.arange(1, 6))\n",
    "        \n",
    "    # 2D array used to store posterior\n",
    "    # it's shape is (num of test objects , size of class)\n",
    "    results = np.zeros((X.shape[0], p_ci.size))\n",
    "    clazz_labels = p_ci.index.values\n",
    "    for k in range(X.shape[0]):\n",
    "        for i, label in enumerate(clazz_labels):\n",
    "            \n",
    "            # get prior (train result)\n",
    "            prob = p_ci.loc[label]\n",
    "            \n",
    "            # calculate posterior\n",
    "            for j in range(X.shape[1]):\n",
    "                # default = 0\n",
    "                prob *= p_x_ci.get((i, j, X[k, j]), 0)\n",
    "                \n",
    "            results[k, i] = prob\n",
    "            # the P ( class_i | X_obj_k)\n",
    "            \n",
    "    # choose the predicted label ( maximum )\n",
    "    return clazz_labels[results.argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f35fddcc-3a08-4826-853b-b6f1555b8fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = naive_bayes_predict(X_test, p_ci, p_x_ci)\n",
    "y_pred == y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab274421-b998-4d18-871a-0c6cd6e10871",
   "metadata": {},
   "source": [
    "## Addition\n",
    "\n",
    "for Naive-Bayes  , it is important to choose the classifier according to `feature`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4b7e9-0508-4e9d-9dfe-18fccda5de09",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Conclusion\n",
    " pros : \n",
    "1.  ez logic\n",
    "2. low cost in calculating :  all the needed data has been calculated during training\n",
    "3. resist to noise and non-relevant features\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. assume the independency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0231ba3-0183-44ee-90f6-4db55c52b0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
