{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a5a0b6-255c-491b-954b-258da4640bcf",
   "metadata": {},
   "source": [
    "# Make your own neural network \n",
    "\n",
    "here's the instruction of how to mk my own neural network\n",
    "\n",
    "**Skeleton Code**\n",
    "\n",
    "1. Initialization\n",
    "2. Train\n",
    "3. Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512593a-0316-48f8-92e5-cd61772dd23f",
   "metadata": {},
   "source": [
    "```python\n",
    "# A simple example of 3 layors \n",
    "\n",
    "class neuralNetwork:\n",
    "\n",
    "    # initialize the neural network\n",
    "    def __init__(self,inputnodes,hiddennodes,outputnodes,learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.lr = learningrate\n",
    "\n",
    "    # training funcion\n",
    "    def train(): pass\n",
    "\n",
    "    # query the neural network\n",
    "    def query(): pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca0b37-dbdc-41e5-b51d-a4d687b9826c",
   "metadata": {},
   "source": [
    "---\n",
    "add the initializing data of node_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a8544-4087-4cd9-96af-80152600d6f8",
   "metadata": {},
   "source": [
    "```python\n",
    "# number of nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.3\n",
    "\n",
    "#create instance\n",
    "n=neuralNetwork(input_nodes,hidden_notes,output_nodes,learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014664c-e67a-4061-b8f9-f6012d227c88",
   "metadata": {},
   "source": [
    "---\n",
    "Next , use numpy module to initailze the input-hidden weight matrix and hidden-output matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d61fb-db5e-4bf8-935f-b24c82db9fc0",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy\n",
    "\n",
    "\n",
    "self.wih = (numpy.random.rand(self.hnodes , self.inodes)-0.5)\n",
    "# the arthimetic operation will work on every elements\n",
    "\n",
    "self.who = (numpy.random.rand(self.inodes , self.onodes)-0.5)\n",
    "\n",
    "# or use a more \"complex\" way\n",
    "self.wih = numpy.random.normal(0.0,pow(self.hnodes,-0.5),(self.hnodes,self.inodes))\n",
    "self.who = numpy.random.normal(0.0,pow(self.onodes,-0.5),(self.onodes,self.hnodes))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1230c-e663-4fd6-a0c5-8ce3b8d6d2cd",
   "metadata": {},
   "source": [
    "---\n",
    "Accomplish the **Query function** first , it is ez and we need it for training : $X_hidden = W_ih \\cdot I$ , go on calculating we'll get the answer\n",
    "\n",
    "And for the output of each layer , $O_hidden = sigmoid(X_hidden)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241633d-7058-4a61-9269-4f49ef2348af",
   "metadata": {},
   "source": [
    "```python\n",
    "import scipy.special\n",
    "# for calling sigmoid\n",
    "# define activation function in the class instead\n",
    "\n",
    "self.activation_function = lambda x:scipy.special.expit(x)\n",
    "\n",
    "\n",
    "\n",
    "hidden_inputs = numpy.dot(self.wih,inputs)\n",
    "\n",
    "hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "\n",
    "final_inputs = numpy.dot(self.who,hidden_outputs)\n",
    "final_outputs = self.activation_funciotn(final_inputs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9918bf7-5609-49a4-9fcc-2e89b11097c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's see what we've got so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d969116c-006f-42d6-8651-84ed9a0eab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "# initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes,\n",
    "            learningrate): # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        \n",
    "# link weight matrices, wih and who\n",
    "# weights inside the arrays are w_i_j, where link is from node i tonode j in the next layer \n",
    "# w11 w21\n",
    "# w12 w22 etc\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes)) \n",
    "        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "    \n",
    "# learning rate\n",
    "        self.lr = learningrate\n",
    "# activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "# train the neural network\n",
    "    def train():\n",
    "        pass\n",
    "\n",
    "    \n",
    "# query the neural network\n",
    "    def query(self, inputs_list):# convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "    # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "    # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a908a-e02d-4e5a-81c4-b398cba0e3b6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "So the next step is to ***train*** our network:\n",
    "1. work on the output of training data\n",
    "2. calculate **cost** to guide the updating fo weight matrix\n",
    "\n",
    "```python\n",
    "# train function\n",
    "def train(self,inputs_list,targets_lists):\n",
    "    # convert inputs to matrix \n",
    "    inputs = numpy.array(inputs_list,ndmin=2).T #transpsition\n",
    "    targets =numpy.array(targets_list,ndmin=2).T\n",
    "\n",
    "    # calculate signals into hidden layer\n",
    "    hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "    hidden_outputs = self.activation_function(hidden_inputs)\n",
    "    \n",
    "    # calculate signals into final output layer\n",
    "    final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layer\n",
    "    final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "    \n",
    "    # output layer error is the (target - actual)\n",
    "    output_errors = targets - final_outputs\n",
    "    # hidden layer error is the output_errors, split by weights, recombinedat hidden nodes \n",
    "    hidden_errors = numpy.dot(self.who.T, output_errors)\n",
    "    \n",
    "    # update the weights for the links between the hidden and outputlayers\n",
    "    self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 -final_outputs)), numpy.transpose(hidden_outputs)) \n",
    "    \n",
    "    # update the weightfor the links between the input and hidden layers\n",
    "    self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 -hidden_outputs)), numpy.transpose(inputs)) \n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e0113-121f-457c-b5f6-5f5cbf8b0fd8",
   "metadata": {},
   "source": [
    "---\n",
    "So we've got the entire code for the little network here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d460724-0391-4ff8-b8aa-d2fa6ccc4494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62707948],\n",
       "       [0.56141421],\n",
       "       [0.42311942]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python notebook for Make Your Own Neural Network\n",
    "# (c) Tariq Rashid, 2016\n",
    "# license is GPLv2\n",
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "# test query (doesn't mean anything useful yet)\n",
    "n.query([1.0, 0.5, -1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d5443-7d33-45f4-a669-65498a7904ef",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Now step to the how to train the network using data\n",
    "\n",
    "use `.CSV` file to train . Python can easily read this kind of plain-text file .\n",
    "\n",
    "eg.\n",
    "\n",
    "```python\n",
    "data_file = open (\"path\",'r') # 'r' stands fot reading mode\n",
    "data_list = date_file.readlines()\n",
    "date_file.close()\n",
    "```\n",
    "\n",
    "to get the value inside , use `split` function\n",
    "\n",
    "eg.\n",
    "\n",
    "```python\n",
    "all_values = data_list[0].split(',')\n",
    "image_array = numpy.asfarray( all_values[1:].reshape(28,28)) # use slide to get rid of the first value , which is the label\n",
    "```\n",
    "\n",
    "---\n",
    "### But before training , we need extra effort to scale the datas to make them work\n",
    "\n",
    "eg.RGB(0-255) -> (0.1,i)\n",
    "\n",
    "- we need to avoid 0 as input , 1 as output\n",
    "```python\n",
    "scaled_input = ( numpy.asfarray(all_values[1:])/255.0 * 0.99 ) + 0.01\n",
    "```\n",
    "\n",
    "- and here's how to scale target : \n",
    "```python\n",
    "oondes = 10 #the number of output_node\n",
    "targets = numpy.zeros(onodes) + 0.01 # avoid 0\n",
    "\n",
    "label = all_values [0]\n",
    "targets [int(label)] = 0.99 # avoid 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ab25b-cff6-4701-ba45-1be508d6db16",
   "metadata": {},
   "source": [
    "## see what we've got so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72fdb684-40d5-44ff-a643-9570389e4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "# library for plotting arrays\n",
    "import matplotlib.pyplot\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 400\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "# load the mnist training data CSV file into a list\n",
    "training_data_file = open(\"mnist_dataset/mnist_train_100.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "# go through all records in the training data set\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "    # no more asfarray in new version of numpy !!!\n",
    "    \n",
    "\n",
    "    # create the target output values (all 0.01, except the desired label whichis 0.99) \n",
    "    targets = numpy.zeros(output_nodes) + 0.01\n",
    "    # all_values[0] is the target label for this record\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    n.train(inputs, targets)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef392aa-6483-41ca-9b19-3e5ceb0f4ef5",
   "metadata": {},
   "source": [
    "---\n",
    "Cool! training finished , now steo to testing\n",
    "\n",
    "## Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5146df48-e3a5-4be6-9887-186bb66c104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data_file = open (\"mnist_dataset/mnist_test_10.csv\" , 'r' )\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3862494-baa8-4f8f-9039-5238fdcd0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asarray(all_values[1:],dtype = float)) / 255.0 * 0.99 + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd4d0180-805c-47a2-a6ea-2074e9c68c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "performance =  0.5\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "print (scorecard)\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa157bf-a24e-401a-b6df-9ecb9b80f01e",
   "metadata": {},
   "source": [
    "---\n",
    "It it not behaving good enough\n",
    "\n",
    "## Give it a real TRAIN\n",
    "\n",
    "using **large** data to train it again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5a1722c-a1c3-4f50-abe3-646c7299503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist training data CSV file into a list\n",
    "training_data_file = open(\"mnist_dataset/mnist_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "# go through all records in the training data set\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asarray(all_values[1:], dtype=float) / 255.0 * 0.99) + 0.01\n",
    "\n",
    "    # no more asfarray in new version of numpy !!!\n",
    "    \n",
    "\n",
    "    # create the target output values (all 0.01, except the desired label whichis 0.99) \n",
    "    targets = numpy.zeros(output_nodes) + 0.01\n",
    "    # all_values[0] is the target label for this record\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    n.train(inputs, targets)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d33d3c5-0565-4e72-802f-418f06eef6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data_file = open (\"mnist_dataset/mnist_test.csv\" , 'r' )\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9758e3da-ae5e-4d2a-b499-227d99a867b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.9574425574425575\n"
     ]
    }
   ],
   "source": [
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asarray(all_values[1:],dtype = float)) / 255.0 * 0.99 + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass\n",
    "\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bccd1-63aa-4f6e-b152-b9cffaad4490",
   "metadata": {},
   "source": [
    "---\n",
    "we can see the performance is around 95% , relatively high\n",
    "\n",
    "---\n",
    "\n",
    "## Recognize from own image .\n",
    "\n",
    "By converting image to an array ( input_list ) size of 784:\n",
    "\n",
    "1. read image , convert to black_white , use one number to replace rgb_array\n",
    "2. process noise (optional) : for isolated black dots , erase it\n",
    "3. zoom and squeeze : zoom to the \"number\"(optional) , squeeze it to shape **28 * 28**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88df370c-b2cb-45b2-83b2-11cb516bbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "def readim_gray(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read an image using matplotlib and return a grayscale ndarray.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the image file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_gray : np.ndarray\n",
    "        2D grayscale image array (dtype float32, range [0,1]).\n",
    "    \"\"\"\n",
    "    img = plt.imread(path)\n",
    "\n",
    "    # If it's RGB or RGBA, convert to grayscale\n",
    "    if img.ndim == 3:\n",
    "        img = img[..., :3]  # drop alpha if exists\n",
    "        img_gray = (\n",
    "            0.2989 * img[..., 0] +\n",
    "            0.5870 * img[..., 1] +\n",
    "            0.1140 * img[..., 2]\n",
    "        )\n",
    "    else:\n",
    "        img_gray = img\n",
    "\n",
    "\n",
    "    return img_gray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e01528bd-a6e5-4c61-b9c6-30a1fa9daa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_mnist_style(img: np.ndarray,\n",
    "                           target_size: int = 28,\n",
    "                           box_size: int = 20,\n",
    "                           padding_ratio: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess a grayscale image to MNIST-like 28x28 centered image.\n",
    "\n",
    "    Steps:\n",
    "    1. Normalize to [0,1].\n",
    "    2. Detect which polarity corresponds to the digit (auto-invert if needed).\n",
    "    3. Crop bounding box around the digit, add small padding.\n",
    "    4. Resize proportionally so the longest side fits `box_size`.\n",
    "    5. Place the resized content into a target_size x target_size canvas centered.\n",
    "    6. Compute center-of-mass and shift so mass is centered at (target_size/2).\n",
    "    7. Normalize to [0.01, 0.99] and return float32.\n",
    "\n",
    "    Note: requires skimage.transform.resize for best results. If scipy.ndimage.shift is available\n",
    "    it will be used for subpixel shift; otherwise an integer roll fallback is used.\n",
    "    \"\"\"\n",
    "    # 1. normalize dtype and range\n",
    "    img = img.astype(np.float32)\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0\n",
    "\n",
    "    # 2. auto-detect polarity: choose the representation where digit area is smaller\n",
    "    #    so that \"digit\" has larger intensity after possible inversion.\n",
    "    thresh = img.mean()\n",
    "    area_dark = np.sum(img < thresh)\n",
    "    area_bright = np.sum(img > thresh)\n",
    "    if area_dark < area_bright:\n",
    "        proc = 1.0 - img  # digit was dark -> invert so digit becomes bright\n",
    "    else:\n",
    "        proc = img.copy()  # digit already bright\n",
    "\n",
    "    # 3. bounding box\n",
    "    thresh = max(proc.max() * 0.1, 0.05)  \n",
    "    mask = proc > thresh\n",
    "     # keep a fraction of max to avoid tiny noise\n",
    "    rows = np.any(mask, axis=1)\n",
    "    cols = np.any(mask, axis=0)\n",
    "    if not rows.any() or not cols.any():\n",
    "        raise ValueError(\"No digit found — try lowering threshold or check the image.\")\n",
    "\n",
    "    r_inds = np.where(rows)[0]\n",
    "    c_inds = np.where(cols)[0]\n",
    "    top, bottom = int(r_inds[0]), int(r_inds[-1])\n",
    "    left, right = int(c_inds[0]), int(c_inds[-1])\n",
    "\n",
    "    crop = proc[top:bottom+1, left:right+1]\n",
    "\n",
    "    # add small padding to the crop\n",
    "    pad = max(1, int(max(crop.shape) * padding_ratio))\n",
    "    crop = np.pad(crop, ((pad, pad), (pad, pad)), mode='constant', constant_values=0.0)\n",
    "\n",
    "    # 4. resize keeping aspect ratio so longer side -> box_size\n",
    "    try:\n",
    "        from skimage.transform import resize\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"skimage is required for resizing. Install scikit-image.\") from e\n",
    "\n",
    "    h, w = crop.shape\n",
    "    scale = box_size / max(h, w)\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    resized = resize(crop, (new_h, new_w), anti_aliasing=True)\n",
    "\n",
    "    # 5. place into canvas centered\n",
    "    canvas = np.zeros((target_size, target_size), dtype=np.float32)\n",
    "    start_y = (target_size - new_h) // 2\n",
    "    start_x = (target_size - new_w) // 2\n",
    "    canvas[start_y:start_y+new_h, start_x:start_x+new_w] = resized\n",
    "\n",
    "    # 6. compute center of mass and shift so the CoM is at the image center\n",
    "    # prefer scipy.ndimage.shift for subpixel shift, fallback to integer shift\n",
    "    cy = cx = None\n",
    "    total = canvas.sum()\n",
    "    if total == 0:\n",
    "        # empty after preprocess (shouldn't happen because we checked mask), return canvas\n",
    "        final = canvas\n",
    "    else:\n",
    "        # compute center of mass\n",
    "        ys, xs = np.indices(canvas.shape)\n",
    "        cy = (ys * canvas).sum() / total\n",
    "        cx = (xs * canvas).sum() / total\n",
    "\n",
    "        shift_y = (target_size - 1) / 2.0 - cy\n",
    "        shift_x = (target_size - 1) / 2.0 - cx\n",
    "\n",
    "        # try scipy shift for subpixel accuracy\n",
    "        try:\n",
    "            from scipy.ndimage import shift as ndi_shift\n",
    "            final = ndi_shift(canvas, shift=(shift_y, shift_x), order=1, mode='constant', cval=0.0)\n",
    "        except Exception:\n",
    "            # integer fallback using roll + zeroing wrapped areas\n",
    "            ry = int(round(shift_y))\n",
    "            rx = int(round(shift_x))\n",
    "            final = np.roll(canvas, shift=ry, axis=0)\n",
    "            final = np.roll(final, shift=rx, axis=1)\n",
    "            # zero wrapped regions\n",
    "            if ry > 0:\n",
    "                final[:ry, :] = 0.0\n",
    "            elif ry < 0:\n",
    "                final[ry:, :] = 0.0\n",
    "            if rx > 0:\n",
    "                final[:, :rx] = 0.0\n",
    "            elif rx < 0:\n",
    "                final[:, rx:] = 0.0\n",
    "\n",
    "    # 7. normalize to [0,1] then map to [0.01, 0.99]\n",
    "    final = final.astype(np.float32)\n",
    "    final -= final.min()\n",
    "    # if final.max() > 0:\n",
    "    #     final /= final.max()\n",
    "\n",
    "    final = final * 0.99 + 0.01\n",
    "    return final.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57c1c611-6019-4de1-aea5-f5b5bbac7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a predict_out funcion\n",
    "def predict_out(matrix: np.ndarray) -> int:\n",
    "    values = matrix.flatten()  # 展平成一维\n",
    "    out = -1\n",
    "    max_val = -float(\"inf\")\n",
    "    for i in range(values.size):\n",
    "        if values[i] > max_val:\n",
    "            max_val = values[i]\n",
    "            out = i\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddb1fc09-254f-441b-8630-b1ad527e14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.07269545 0.15386224 0.10121042 0.06854226\n",
      "  0.08530078 0.12853648 0.17778005 0.19966862 0.10868453 0.02450573\n",
      "  0.01023324 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.12645425 0.39652184 0.4633912  0.46258053\n",
      "  0.47266275 0.48156324 0.4722592  0.48191917 0.47987673 0.14418925\n",
      "  0.02120047 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.03764756 0.1190823  0.15886919 0.1673724\n",
      "  0.16670728 0.15844712 0.14160368 0.20848827 0.5718385  0.18594515\n",
      "  0.02411887 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.0133693  0.02462525 0.02905039 0.02784042\n",
      "  0.03120168 0.0381918  0.0311161  0.42916405 0.49992272 0.10252096\n",
      "  0.01220988 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01035837 0.01130036 0.01125649 0.01099758\n",
      "  0.01398323 0.01706201 0.05010641 0.6540725  0.3140834  0.04541889\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01035159\n",
      "  0.01500306 0.01612025 0.23586997 0.6682199  0.16917716 0.01922348\n",
      "  0.01039166 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.0107789\n",
      "  0.01655054 0.01468907 0.5013421  0.5096098  0.09867711 0.01600785\n",
      "  0.01087315 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01307887\n",
      "  0.01567601 0.05911623 0.6708064  0.28062275 0.04129946 0.01332373\n",
      "  0.01053701 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01288605\n",
      "  0.01886059 0.22445898 0.6623281  0.148313   0.01797231 0.01197397\n",
      "  0.01020502 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01363152\n",
      "  0.01644456 0.3977792  0.5038571  0.09632672 0.01784586 0.01224651\n",
      "  0.01015813 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01519755\n",
      "  0.05262285 0.5236891  0.30933055 0.05098893 0.01377569 0.010981\n",
      "  0.01005534 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01249189 0.02127584\n",
      "  0.14384824 0.57921493 0.2124248  0.03466353 0.01317358 0.01045347\n",
      "  0.01001089 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01382583 0.01952667\n",
      "  0.22998777 0.5590517  0.17039588 0.03266684 0.01316511 0.01023063\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01398955 0.01730451\n",
      "  0.30749843 0.49536154 0.12747608 0.02499221 0.01255648 0.01019937\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.015186   0.03325643\n",
      "  0.4011229  0.42113498 0.08680391 0.0159882  0.01100093 0.01007088\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.02019012 0.05042969\n",
      "  0.46951652 0.36777264 0.06799222 0.01355527 0.01061715 0.01004744\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01717487 0.03925209\n",
      "  0.30493313 0.21979553 0.04910931 0.0122047  0.01017554 0.01004744\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01001457 0.01004178 0.01192636 0.01613199\n",
      "  0.0695799  0.05179255 0.01932137 0.01069868 0.01004665 0.01002236\n",
      "  0.01000292 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01000179 0.01000513 0.01016945 0.010366\n",
      "  0.01346285 0.0124398  0.01068483 0.01007137 0.01000573 0.01000274\n",
      "  0.01000036 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGUFJREFUeJzt3X9oVff9x/HXNcZbTW8uy9Lk3tQYgmg31AlVp4aqsWBmYFabjdkWtviPtDMKkhaZk2G2P0xx1PlHVse6kSnfOv1j1glKbYYmtjg3GyyKE0kxzoi5ZAabG1O9Lvr5/iFedk38ca73+s69eT7gQO+55+P5eHrM0+O991yfc84JAAADY6wnAAAYvYgQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwM9Z6Ave7c+eOrly5okAgIJ/PZz0dAIBHzjn19/erpKREY8Y8/FpnxEXoypUrKi0ttZ4GAOAJdXV1aeLEiQ/dZsRFKBAISLo7+fz8fOPZAAC8ikajKi0tjf88f5i0Rej999/Xr3/9a3V3d2vatGnavn27FixY8Mhx9/4JLj8/nwgBQAZ7nJdU0vLGhL1792r9+vXatGmTTp06pQULFqi6ulqXLl1Kx+4AABnKl467aM+dO1cvvviiduzYEV/37W9/WytWrFBjY+NDx0ajUQWDQfX19XElBAAZyMvP8ZRfCd26dUvt7e2qqqpKWF9VVaXjx48P2T4WiykajSYsAIDRIeURunr1qm7fvq3i4uKE9cXFxYpEIkO2b2xsVDAYjC+8Mw4ARo+0fVj1/heknHPDvki1ceNG9fX1xZeurq50TQkAMMKk/N1xhYWFysnJGXLV09PTM+TqSJL8fr/8fn+qpwEAyAApvxIaN26cZs2apZaWloT1LS0tqqioSPXuAAAZLC2fE6qvr9ePf/xjzZ49W/Pnz9fvf/97Xbp0SW+99VY6dgcAyFBpidDKlSvV29urX/3qV+ru7tb06dN16NAhlZWVpWN3AIAMlZbPCT0JPicEAJnN9HNCAAA8LiIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYCblEWpoaJDP50tYQqFQqncDAMgCY9Pxi06bNk1/+9vf4o9zcnLSsRsAQIZLS4TGjh3L1Q8A4JHS8ppQR0eHSkpKVF5ertdee00XLlx44LaxWEzRaDRhAQCMDimP0Ny5c7Vr1y4dPnxYH3zwgSKRiCoqKtTb2zvs9o2NjQoGg/GltLQ01VMCAIxQPuecS+cOBgYGNHnyZG3YsEH19fVDno/FYorFYvHH0WhUpaWl6uvrU35+fjqnBgBIg2g0qmAw+Fg/x9PymtD/ysvL04wZM9TR0THs836/X36/P93TAACMQGn/nFAsFtO5c+cUDofTvSsAQIZJeYTeeecdtbW1qbOzU//4xz/0wx/+UNFoVLW1taneFQAgw6X8n+MuX76s119/XVevXtVzzz2nefPm6cSJEyorK0v1rgAAGS7lEdqzZ0+qf0kAQJbi3nEAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJm0f6kdstf169c9j7l69arnMcl8w26yX5R4+/Ztz2MGBgY8j4lGo57H3Lx50/OY559/3vMYSfrGN77heUxOTk5S+8LoxpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHAXbSStv7/f85jm5mbPYy5fvux5TLIGBwc9j0nmztvJ3HE6mf0kq7Gx0fOYZO7YPWYMfw8e7TgDAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAUSSssLPQ8Zs2aNZ7HDAwMeB5z584dz2OSlcxNOHNzcz2PuXr1qucxtbW1nsdIyd2c1jmX1L4wunElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamSNrYsd5Pn2RuevrNb37T8xifz+d5jJTcTTiTGZPM/LZt2+Z5zAsvvOB5jCSFQiHPY3JycpLaF0Y3roQAAGaIEADAjOcIHTt2TMuWLVNJSYl8Pp/279+f8LxzTg0NDSopKdH48eNVWVmps2fPpmq+AIAs4jlCAwMDmjlzppqamoZ9fuvWrdq2bZuampp08uRJhUIhLVmyJKkvyQIAZDfPryxXV1erurp62Oecc9q+fbs2bdqkmpoaSdLOnTtVXFys3bt3680333yy2QIAskpKXxPq7OxUJBJRVVVVfJ3f79eiRYt0/PjxYcfEYjFFo9GEBQAwOqQ0QpFIRJJUXFycsL64uDj+3P0aGxsVDAbjS2lpaSqnBAAYwdLy7rj7PwPhnHvg5yI2btyovr6++NLV1ZWOKQEARqCUflj13gfcIpGIwuFwfH1PT8+Qq6N7/H6//H5/KqcBAMgQKb0SKi8vVygUUktLS3zdrVu31NbWpoqKilTuCgCQBTxfCV2/fl1ffvll/HFnZ6e++OILFRQUaNKkSVq/fr22bNmiKVOmaMqUKdqyZYsmTJigN954I6UTBwBkPs8R+vzzz7V48eL44/r6eklSbW2t/vSnP2nDhg26ceOG1qxZo2vXrmnu3Ln65JNPFAgEUjdrAEBW8Llk7r6YRtFoVMFgUH19fcrPz7eeDkaZp/XHIRaLeR4TDAY9j/nDH/7geYwk/ehHP/I8htd2cY+Xn+PcOw4AYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmUvrNqgAeT3t7u+cx//3vfz2PmTdvnucxkpSbm5vUOMArroQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcwBQw8Jvf/MbzmFdeecXzmMLCQs9jJMnn8yU1DvCKKyEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAw3MEVWcs49tX3dvHnT85i//OUvnsccOHDA85i8vDzPY4CniSshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzAF/kcyNz49ffq05zE5OTmex3znO9/xPGbs2OT+iPt8vqTGAV5xJQQAMEOEAABmPEfo2LFjWrZsmUpKSuTz+bR///6E51etWiWfz5ewzJs3L1XzBQBkEc8RGhgY0MyZM9XU1PTAbZYuXaru7u74cujQoSeaJAAgO3l+1bK6ulrV1dUP3cbv9ysUCiU9KQDA6JCW14RaW1tVVFSkqVOnavXq1erp6XngtrFYTNFoNGEBAIwOKY9QdXW1PvzwQx05ckTvvfeeTp48qZdfflmxWGzY7RsbGxUMBuNLaWlpqqcEABihUv45oZUrV8b/e/r06Zo9e7bKysp08OBB1dTUDNl+48aNqq+vjz+ORqOECABGibR/WDUcDqusrEwdHR3DPu/3++X3+9M9DQDACJT2zwn19vaqq6tL4XA43bsCAGQYz1dC169f15dffhl/3NnZqS+++EIFBQUqKChQQ0ODfvCDHygcDuvixYv6+c9/rsLCQr366qspnTgAIPN5jtDnn3+uxYsXxx/fez2ntrZWO3bs0JkzZ7Rr1y599dVXCofDWrx4sfbu3atAIJC6WQMAsoLnCFVWVj70Jo+HDx9+ogkBlpK5cefBgwc9j1m+fLnnMcFg0PMYYKTj3nEAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwk/ZvVgUySSwW8zzmn//8p+cxP/nJTzyPeeaZZzyPSeau4MDTxJUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5giKznnkhr3n//8x/OYS5cueR7zwgsveB6Tk5PjeQww0nElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamGPGSvRlpMi5fvux5TF5enucx+fn5nsf4fD7PY4CRjishAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzBFVkr2Zp+dnZ2ex5SXl3se8+yzz3oek8zviZueYqTjSggAYIYIAQDMeIpQY2Oj5syZo0AgoKKiIq1YsULnz59P2MY5p4aGBpWUlGj8+PGqrKzU2bNnUzppAEB28BShtrY21dXV6cSJE2ppadHg4KCqqqo0MDAQ32br1q3atm2bmpqadPLkSYVCIS1ZskT9/f0pnzwAILN5emPCxx9/nPC4ublZRUVFam9v18KFC+Wc0/bt27Vp0ybV1NRIknbu3Kni4mLt3r1bb775ZupmDgDIeE/0mlBfX58kqaCgQNLddxZFIhFVVVXFt/H7/Vq0aJGOHz8+7K8Ri8UUjUYTFgDA6JB0hJxzqq+v10svvaTp06dLkiKRiCSpuLg4Ydvi4uL4c/drbGxUMBiML6WlpclOCQCQYZKO0Nq1a3X69Gn9+c9/HvLc/Z9NcM498PMKGzduVF9fX3zp6upKdkoAgAyT1IdV161bpwMHDujYsWOaOHFifH0oFJJ094ooHA7H1/f09Ay5OrrH7/fL7/cnMw0AQIbzdCXknNPatWu1b98+HTlyZMgnxcvLyxUKhdTS0hJfd+vWLbW1tamioiI1MwYAZA1PV0J1dXXavXu3/vrXvyoQCMRf5wkGgxo/frx8Pp/Wr1+vLVu2aMqUKZoyZYq2bNmiCRMm6I033kjLbwAAkLk8RWjHjh2SpMrKyoT1zc3NWrVqlSRpw4YNunHjhtasWaNr165p7ty5+uSTTxQIBFIyYQBA9vAUIefcI7fx+XxqaGhQQ0NDsnMCntjg4GBS486dO+d5zKRJkzyPycvL8zxmzBjusoXsw1kNADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0l9syrwND3O3dvvd/v27aT2lczXy0+bNs3zmJycHM9jgGzElRAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYbmGLE8/l8nseMGZPc36++973veR4zefJkz2OSuYFpMscBGOm4EgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHADU4x4ydy4Mzc3N6l9vfLKK57HJHMz0nHjxnkeA2QjroQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcwBRZKZmbnkrShAkTntq+AHAlBAAwRIQAAGY8RaixsVFz5sxRIBBQUVGRVqxYofPnzydss2rVKvl8voRl3rx5KZ00ACA7eIpQW1ub6urqdOLECbW0tGhwcFBVVVUaGBhI2G7p0qXq7u6OL4cOHUrppAEA2cHTGxM+/vjjhMfNzc0qKipSe3u7Fi5cGF/v9/sVCoVSM0MAQNZ6oteE+vr6JEkFBQUJ61tbW1VUVKSpU6dq9erV6unpeeCvEYvFFI1GExYAwOjgc865ZAY657R8+XJdu3ZNn376aXz93r179eyzz6qsrEydnZ36xS9+ocHBQbW3t8vv9w/5dRoaGvTLX/5yyPq+vj7l5+cnMzUgacn8ceAt2kCiaDSqYDD4WD/Hk45QXV2dDh48qM8++0wTJ0584Hbd3d0qKyvTnj17VFNTM+T5WCymWCyWMPnS0lIiBBNECHhyXiKU1IdV161bpwMHDujYsWMPDZAkhcNhlZWVqaOjY9jn/X7/sFdIAIDs5ylCzjmtW7dOH330kVpbW1VeXv7IMb29verq6lI4HE56kgCA7OTpjQl1dXX6v//7P+3evVuBQECRSESRSEQ3btyQJF2/fl3vvPOO/v73v+vixYtqbW3VsmXLVFhYqFdffTUtvwEAQObydCW0Y8cOSVJlZWXC+ubmZq1atUo5OTk6c+aMdu3apa+++krhcFiLFy/W3r17FQgEUjZpAEB28PzPcQ8zfvx4HT58+IkmBAAYPbiLNvA/eKcb8HRxA1MAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMjLWewP2cc5KkaDRqPBMAQDLu/fy+9/P8YUZchPr7+yVJpaWlxjMBADyJ/v5+BYPBh27jc4+Tqqfozp07unLligKBgHw+X8Jz0WhUpaWl6urqUn5+vtEM7XEc7uI43MVxuIvjcNdIOA7OOfX396ukpERjxjz8VZ8RdyU0ZswYTZw48aHb5Ofnj+qT7B6Ow10ch7s4DndxHO6yPg6PugK6hzcmAADMECEAgJmMipDf79fmzZvl9/utp2KK43AXx+EujsNdHIe7Mu04jLg3JgAARo+MuhICAGQXIgQAMEOEAABmiBAAwExGRej9999XeXm5nnnmGc2aNUuffvqp9ZSeqoaGBvl8voQlFApZTyvtjh07pmXLlqmkpEQ+n0/79+9PeN45p4aGBpWUlGj8+PGqrKzU2bNnbSabRo86DqtWrRpyfsybN89msmnS2NioOXPmKBAIqKioSCtWrND58+cTthkN58PjHIdMOR8yJkJ79+7V+vXrtWnTJp06dUoLFixQdXW1Ll26ZD21p2ratGnq7u6OL2fOnLGeUtoNDAxo5syZampqGvb5rVu3atu2bWpqatLJkycVCoW0ZMmS+H0Is8WjjoMkLV26NOH8OHTo0FOcYfq1tbWprq5OJ06cUEtLiwYHB1VVVaWBgYH4NqPhfHic4yBlyPngMsR3v/td99ZbbyWs+9a3vuV+9rOfGc3o6du8ebObOXOm9TRMSXIfffRR/PGdO3dcKBRy7777bnzdzZs3XTAYdL/73e8MZvh03H8cnHOutrbWLV++3GQ+Vnp6epwk19bW5pwbvefD/cfBucw5HzLiSujWrVtqb29XVVVVwvqqqiodP37caFY2Ojo6VFJSovLycr322mu6cOGC9ZRMdXZ2KhKJJJwbfr9fixYtGnXnhiS1traqqKhIU6dO1erVq9XT02M9pbTq6+uTJBUUFEgavefD/cfhnkw4HzIiQlevXtXt27dVXFycsL64uFiRSMRoVk/f3LlztWvXLh0+fFgffPCBIpGIKioq1Nvbaz01M/f+/4/2c0OSqqur9eGHH+rIkSN67733dPLkSb388suKxWLWU0sL55zq6+v10ksvafr06ZJG5/kw3HGQMud8GHF30X6Y+7/awTk3ZF02q66ujv/3jBkzNH/+fE2ePFk7d+5UfX294czsjfZzQ5JWrlwZ/+/p06dr9uzZKisr08GDB1VTU2M4s/RYu3atTp8+rc8++2zIc6PpfHjQcciU8yEjroQKCwuVk5Mz5G8yPT09Q/7GM5rk5eVpxowZ6ujosJ6KmXvvDuTcGCocDqusrCwrz49169bpwIEDOnr0aMJXv4y28+FBx2E4I/V8yIgIjRs3TrNmzVJLS0vC+paWFlVUVBjNyl4sFtO5c+cUDoetp2KmvLxcoVAo4dy4deuW2traRvW5IUm9vb3q6urKqvPDOae1a9dq3759OnLkiMrLyxOeHy3nw6OOw3BG7Plg+KYIT/bs2eNyc3PdH//4R/evf/3LrV+/3uXl5bmLFy9aT+2pefvtt11ra6u7cOGCO3HihPv+97/vAoFA1h+D/v5+d+rUKXfq1CknyW3bts2dOnXK/fvf/3bOOffuu++6YDDo9u3b586cOeNef/11Fw6HXTQaNZ55aj3sOPT397u3337bHT9+3HV2drqjR4+6+fPnu+effz6rjsNPf/pTFwwGXWtrq+vu7o4vX3/9dXyb0XA+POo4ZNL5kDERcs653/72t66srMyNGzfOvfjiiwlvRxwNVq5c6cLhsMvNzXUlJSWupqbGnT171npaaXf06FEnachSW1vrnLv7ttzNmze7UCjk/H6/W7hwoTtz5oztpNPgYcfh66+/dlVVVe65555zubm5btKkSa62ttZdunTJetopNdzvX5Jrbm6ObzMazodHHYdMOh/4KgcAgJmMeE0IAJCdiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz/w8W6LWLJ0QwLQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#origin_image = readim_gray(\"testimages/6.jpg\")\n",
    "test_image = preprocess_mnist_style(readim_gray(\"testimages/Image00007.png\")).reshape(784) \n",
    "\n",
    "print(test_image.reshape(28,28))\n",
    "\n",
    "plt.imshow(test_image.reshape(28, 28), cmap=\"Greys\",interpolation='None')\n",
    "\n",
    "test_output = n.query(test_image)\n",
    "print(predict_out(test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca312aa1-6b3d-4595-9ef5-f96b9af91cc7",
   "metadata": {},
   "source": [
    "---\n",
    "## Not performing well in real image\n",
    "So it didn't perform well. **Pity**.\n",
    "\n",
    "Is it because I cannot completely process the image into the type that training data have ?\n",
    "Actually the zoom function worked just well\n",
    "\n",
    "by looking into the array generated , it seems that compared to the *standard data* , my data have lower values\n",
    "\n",
    "**Maybe I can get it better by increasing the values ?**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4aae9a5f-a4ee-45aa-a9fb-c9fab1cb07eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.53634775 0.5769311  0.5506052  0.5342711\n",
      "  0.5426504  0.56426823 0.58889    0.5998343  0.55434227 0.02450573\n",
      "  0.01023324 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.5632271  0.84913045 0.8658478  0.8656451\n",
      "  0.8681657  0.87039083 0.8680648  0.8704798  0.8699692  0.5720946\n",
      "  0.02120047 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.03764756 0.5595411  0.5794346  0.58368623\n",
      "  0.58335364 0.5792235  0.57080185 0.80212206 0.8929596  0.5929726\n",
      "  0.02411887 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.0133693  0.02462525 0.02905039 0.02784042\n",
      "  0.03120168 0.0381918  0.0311161  0.857291   0.8749807  0.5512605\n",
      "  0.01220988 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01035837 0.01130036 0.01125649 0.01099758\n",
      "  0.01398323 0.01706201 0.05010641 0.91351813 0.82852083 0.04541889\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01035159\n",
      "  0.01500306 0.01612025 0.8089675  0.917055   0.5845886  0.01922348\n",
      "  0.01039166 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.0107789\n",
      "  0.01655054 0.01468907 0.8753355  0.8774024  0.5493386  0.01600785\n",
      "  0.01087315 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01307887\n",
      "  0.01567601 0.05911623 0.9177016  0.8201557  0.04129946 0.01332373\n",
      "  0.01053701 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01288605\n",
      "  0.01886059 0.80611473 0.91558206 0.5741565  0.01797231 0.01197397\n",
      "  0.01020502 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01363152\n",
      "  0.01644456 0.8494448  0.8759643  0.54816335 0.01784586 0.01224651\n",
      "  0.01015813 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01009896 0.01519755\n",
      "  0.05262285 0.88092226 0.8273326  0.05098893 0.01377569 0.010981\n",
      "  0.01005534 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01249189 0.02127584\n",
      "  0.5719241  0.89480376 0.8031062  0.03466353 0.01317358 0.01045347\n",
      "  0.01001089 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01382583 0.01952667\n",
      "  0.80749696 0.88976294 0.5851979  0.03266684 0.01316511 0.01023063\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01398955 0.01730451\n",
      "  0.8268746  0.8738404  0.56373805 0.02499221 0.01255648 0.01019937\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.015186   0.03325643\n",
      "  0.8502807  0.85528374 0.54340196 0.0159882  0.01100093 0.01007088\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.02019012 0.05042969\n",
      "  0.8673791  0.84194314 0.5339961  0.01355527 0.01061715 0.01004744\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01003091 0.01008863 0.01717487 0.03925209\n",
      "  0.82623327 0.80494887 0.04910931 0.0122047  0.01017554 0.01004744\n",
      "  0.0100062  0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01001457 0.01004178 0.01192636 0.01613199\n",
      "  0.5347899  0.05179255 0.01932137 0.01069868 0.01004665 0.01002236\n",
      "  0.01000292 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01000179 0.01000513 0.01016945 0.010366\n",
      "  0.01346285 0.0124398  0.01068483 0.01007137 0.01000573 0.01000274\n",
      "  0.01000036 0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]\n",
      " [0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01       0.01       0.01\n",
      "  0.01       0.01       0.01       0.01      ]]\n",
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reinforce(img: np.ndarray) -> np.ndarray:\n",
    "    # Copy to avoid in-place modification\n",
    "    reinforced = img.copy()\n",
    "    mask = reinforced > 0.06\n",
    "    reinforced[mask] += (1 - reinforced[mask]) * 0.5\n",
    "    return reinforced\n",
    "\n",
    "test_image = reinforce(test_image)\n",
    "\n",
    "print(test_image.reshape(28, 28))\n",
    "plt.imshow(test_image.reshape(28, 28), cmap=\"Greys\", interpolation=\"none\")\n",
    "\n",
    "test_output = n.query(test_image)\n",
    "print(predict_out(test_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860902ed-d083-43f0-a6b5-9d66c4280c24",
   "metadata": {},
   "source": [
    "## Great ! \n",
    "**So it proved that the recognision problem is only because the line of the number is too thin !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a200aa-401a-4c08-aa38-4139d53986c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
