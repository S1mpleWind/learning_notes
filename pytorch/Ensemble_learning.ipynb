{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebb5b3b-4908-4c97-8053-e12a462d50a8",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "By combining many single models into a integrated model , which is called **Ensemble Learning** , we can get models having **good generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f1680-8aed-4080-9816-2a34305195bf",
   "metadata": {},
   "source": [
    "## Algorithms Types\n",
    "\n",
    "### 1.Baggling\n",
    "\n",
    "randomly sample from the training data , train different models base on the subsets , and combine their results.\n",
    "eg. random forest\n",
    "\n",
    "### 2.Boosting\n",
    "iterate training models . In each round of training , pay special attention to the samples that are not well predicted in previous turn.\n",
    "The target of each round is to make up for the disadvantage of previous model .\n",
    "\n",
    "eg. AdaBoost , Gradiant Boosting , XGBoost\n",
    "\n",
    "To do so , add weight to the wrong-predicted samples\n",
    "\n",
    "### 3.Stacking \n",
    "By stacking the models , use the output of the previous model as next one's input \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90d68d-0879-4f36-be73-ed72ea98a937",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "`Adaboost`(Adaptive Boosting) , steps : \n",
    "\n",
    "1. Initialize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
